name: Integration Tests

on:
  pull_request:
    branches: [main]
    paths:
      - '**.tf'
      - 'tests/**'
      - '.github/workflows/integration-test.yml'
  push:
    branches: [main]
    paths:
      - '**.tf'
      - 'tests/**'
  schedule:
    - cron: '0 6 * * 1'  # Weekly Monday 6am UTC
  workflow_dispatch:
    inputs:
      skip_destroy:
        description: 'Skip destroy step (for debugging)'
        required: false
        type: boolean
        default: false

# CRITICAL: Global concurrency group - only ONE K8s test at a time
# This prevents autoscaler collisions from parallel K8s tests
concurrency:
  group: k8s-integration-test-global
  cancel-in-progress: false  # Queue, don't cancel (prevents orphaned resources)

permissions:
  contents: read
  pull-requests: write

jobs:
  check-credentials:
    name: Check Credentials
    runs-on: ubuntu-latest
    outputs:
      has_credentials: ${{ steps.check.outputs.has_credentials }}
    steps:
      - name: Check for zCompute credentials
        id: check
        env:
          ZCOMPUTE_ENDPOINT: ${{ secrets.ZCOMPUTE_ENDPOINT }}
          ZCOMPUTE_ACCESS_KEY: ${{ secrets.ZCOMPUTE_ACCESS_KEY }}
          ZCOMPUTE_SECRET_KEY: ${{ secrets.ZCOMPUTE_SECRET_KEY }}
        run: |
          if [ -n "$ZCOMPUTE_ENDPOINT" ] && [ -n "$ZCOMPUTE_ACCESS_KEY" ] && [ -n "$ZCOMPUTE_SECRET_KEY" ]; then
            echo "has_credentials=true" >> $GITHUB_OUTPUT
            echo "All zCompute credentials are configured"
          else
            echo "has_credentials=false" >> $GITHUB_OUTPUT
            echo "::warning::Integration tests skipped - zCompute credentials not configured. See CONTRIBUTING.md for setup instructions."

            # Log which credentials are missing
            [ -z "$ZCOMPUTE_ENDPOINT" ] && echo "::warning::Missing secret: ZCOMPUTE_ENDPOINT"
            [ -z "$ZCOMPUTE_ACCESS_KEY" ] && echo "::warning::Missing secret: ZCOMPUTE_ACCESS_KEY"
            [ -z "$ZCOMPUTE_SECRET_KEY" ] && echo "::warning::Missing secret: ZCOMPUTE_SECRET_KEY"
          fi

  test:
    name: Integration Test
    runs-on: ubuntu-latest
    needs: check-credentials
    if: needs.check-credentials.outputs.has_credentials == 'true'
    timeout-minutes: 60

    env:
      TF_VAR_zcompute_endpoint_url: ${{ secrets.ZCOMPUTE_ENDPOINT }}
      TF_VAR_zcompute_access_key: ${{ secrets.ZCOMPUTE_ACCESS_KEY }}
      TF_VAR_zcompute_secret_key: ${{ secrets.ZCOMPUTE_SECRET_KEY }}
      TF_VAR_run_id: ${{ github.run_id }}

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: "1.5.7"
          terraform_wrapper: false

      - name: Generate cluster token
        id: token
        run: |
          TOKEN=$(openssl rand -hex 32)
          echo "::add-mask::$TOKEN"
          echo "token=$TOKEN" >> $GITHUB_OUTPUT

      - name: Generate SSH key
        id: ssh_key
        run: |
          ssh-keygen -t rsa -b 4096 -f /tmp/bastion_ssh_key -N "" -q
          chmod 600 /tmp/bastion_ssh_key
          echo "ssh_public_key=$(cat /tmp/bastion_ssh_key.pub)" >> $GITHUB_OUTPUT

      # ============================================================
      # Phase 1: VPC
      # ============================================================
      - name: VPC - Init
        working-directory: tests/fixtures/vpc
        run: terraform init -input=false

      - name: VPC - Apply
        id: vpc
        working-directory: tests/fixtures/vpc
        run: |
          echo "Deploying VPC: test-k8s-${{ github.run_id }}"
          terraform apply -auto-approve -input=false

          # Capture outputs for downstream fixtures
          echo "vpc_id=$(terraform output -raw vpc_id)" >> $GITHUB_OUTPUT
          echo "private_subnets=$(terraform output -json private_subnets)" >> $GITHUB_OUTPUT
          echo "public_subnets=$(terraform output -json public_subnets)" >> $GITHUB_OUTPUT

          echo "::notice::VPC deployed successfully"

      # ============================================================
      # Phase 2: IAM
      # ============================================================
      - name: IAM - Init
        working-directory: tests/fixtures/iam
        run: terraform init -input=false

      - name: IAM - Apply
        id: iam
        working-directory: tests/fixtures/iam
        run: |
          echo "Deploying IAM: test-k8s-${{ github.run_id }}"
          terraform apply -auto-approve -input=false

          # Capture output for K8s fixture
          echo "instance_profile_name=$(terraform output -raw instance_profile_name)" >> $GITHUB_OUTPUT

          echo "::notice::IAM instance profile deployed successfully"

      - name: Wait for IAM propagation
        run: |
          echo "Waiting 30 seconds for IAM eventual consistency..."
          sleep 30

      # ============================================================
      # Phase 2.5: Bastion (SSH jump host — launched early to boot
      #            while K8s deploys; cluster SG added after K8s)
      # ============================================================
      - name: Bastion - Init
        working-directory: tests/fixtures/bastion
        run: terraform init -input=false

      - name: Bastion - Apply
        id: bastion
        working-directory: tests/fixtures/bastion
        env:
          TF_VAR_vpc_id: ${{ steps.vpc.outputs.vpc_id }}
          TF_VAR_public_subnets: ${{ steps.vpc.outputs.public_subnets }}
          TF_VAR_instance_type: ${{ vars.DEFAULT_INSTANCE_TYPE }}
          TF_VAR_ssh_public_key: ${{ steps.ssh_key.outputs.ssh_public_key }}
          TF_VAR_debug_ssh_public_key: ${{ vars.DEBUG_SSH_PUBLIC_KEY }}
        run: |
          echo "Deploying bastion host: test-k8s-${{ github.run_id }}-bastion"
          terraform apply -auto-approve -input=false

          # Capture outputs
          echo "bastion_ip=$(terraform output -raw bastion_public_ip)" >> $GITHUB_OUTPUT
          echo "ssh_key_name=$(terraform output -raw ssh_key_name)" >> $GITHUB_OUTPUT

          echo "::notice::Bastion host deployed (cluster SG pending K8s deploy)"

      # ============================================================
      # Phase 3: K8s
      # ============================================================
      - name: K8s - Init
        working-directory: tests/fixtures/k8s
        run: terraform init -input=false

      - name: K8s - Apply
        id: k8s
        working-directory: tests/fixtures/k8s
        env:
          TF_VAR_vpc_id: ${{ steps.vpc.outputs.vpc_id }}
          TF_VAR_private_subnets: ${{ steps.vpc.outputs.private_subnets }}
          TF_VAR_iam_instance_profile: ${{ steps.iam.outputs.instance_profile_name }}
          TF_VAR_cluster_token: ${{ steps.token.outputs.token }}
          TF_VAR_default_instance_type: ${{ vars.DEFAULT_INSTANCE_TYPE }}
          TF_VAR_ssh_key_name: ${{ steps.bastion.outputs.ssh_key_name }}
          TF_VAR_debug_ssh_public_key: ${{ vars.DEBUG_SSH_PUBLIC_KEY }}
        run: |
          echo "Deploying K8s cluster: test-k8s-${{ github.run_id }}"

          # Apply with one retry on failure
          if ! terraform apply -auto-approve -input=false; then
            echo "::warning::First apply failed, retrying in 30 seconds..."
            sleep 30
            terraform apply -auto-approve -input=false
          fi

          # Capture outputs
          echo "cluster_name=$(terraform output -raw cluster_name)" >> $GITHUB_OUTPUT
          echo "load_balancer_dns=$(terraform output -raw load_balancer_dns)" >> $GITHUB_OUTPUT
          echo "cluster_security_group_id=$(terraform output -raw cluster_security_group_id)" >> $GITHUB_OUTPUT

          echo "::notice::K8s cluster deployed successfully"

      # ============================================================
      # Phase 3.5: Connect bastion to cluster security group
      # ============================================================
      - name: Bastion - Connect to cluster
        if: steps.k8s.outcome == 'success'
        working-directory: tests/fixtures/bastion
        env:
          TF_VAR_vpc_id: ${{ steps.vpc.outputs.vpc_id }}
          TF_VAR_public_subnets: ${{ steps.vpc.outputs.public_subnets }}
          TF_VAR_instance_type: ${{ vars.DEFAULT_INSTANCE_TYPE }}
          TF_VAR_ssh_public_key: ${{ steps.ssh_key.outputs.ssh_public_key }}
          TF_VAR_debug_ssh_public_key: ${{ vars.DEBUG_SSH_PUBLIC_KEY }}
          TF_VAR_cluster_security_group_id: ${{ steps.k8s.outputs.cluster_security_group_id }}
        run: |
          echo "Adding cluster security group to bastion instance"
          terraform apply -auto-approve -input=false
          echo "::notice::Bastion connected to cluster security group"

      - name: Wait for bastion SSH readiness
        if: steps.k8s.outcome == 'success'
        timeout-minutes: 10
        run: |
          BASTION_IP="${{ steps.bastion.outputs.bastion_ip }}"
          echo "Waiting for bastion SSH at ${BASTION_IP} (timeout: 10 minutes)"

          RETRY=0
          MAX_RETRY=40  # 40 * 15s = 10 min
          while [ $RETRY -lt $MAX_RETRY ]; do
            if ssh -o StrictHostKeyChecking=no -o ConnectTimeout=5 \
                   -i /tmp/bastion_ssh_key "ubuntu@${BASTION_IP}" \
                   "test -f /tmp/bastion-ready" 2>/dev/null; then
              echo "Bastion is ready"
              break
            fi
            echo "Attempt $((RETRY + 1))/$MAX_RETRY: bastion not ready yet..."
            RETRY=$((RETRY + 1))
            sleep 15
          done

          if [ $RETRY -ge $MAX_RETRY ]; then
            echo "::error::Bastion SSH did not become ready within 10 minutes"
            exit 1
          fi

      - name: Copy kubeconfig from control node
        id: kubeconfig
        if: steps.k8s.outcome == 'success'
        timeout-minutes: 15
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.ZCOMPUTE_ACCESS_KEY }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.ZCOMPUTE_SECRET_KEY }}
          AWS_DEFAULT_REGION: us-east-1
        run: |
          BASTION_IP="${{ steps.bastion.outputs.bastion_ip }}"
          LB_DNS="${{ steps.k8s.outputs.load_balancer_dns }}"
          SSH_KEY="/tmp/bastion_ssh_key"
          EC2_ENDPOINT="${{ secrets.ZCOMPUTE_ENDPOINT }}/api/v2/aws/ec2"
          PROXY_CMD="ssh -o StrictHostKeyChecking=no -i ${SSH_KEY} -W %h:%p ubuntu@${BASTION_IP}"

          # Find the oldest running control plane node via zCompute EC2 API.
          # The oldest node is the seed (cluster-init) node — most likely to
          # have K3s fully bootstrapped and the kubeconfig ready.
          echo "Looking for the oldest control plane node..."
          CONTROL_IP=""
          for i in $(seq 1 40); do
            CONTROL_IP=$(aws ec2 describe-instances --no-verify-ssl \
              --endpoint-url "${EC2_ENDPOINT}" \
              --filters "Name=tag:run-id,Values=${{ github.run_id }}" \
                        "Name=tag:zadara.com/k8s/role,Values=control" \
                        "Name=instance-state-name,Values=running" \
              --query 'sort_by(Reservations[].Instances[], &LaunchTime)[0].PrivateIpAddress' \
              --output text 2>/dev/null || true)

            if [ -n "$CONTROL_IP" ] && [ "$CONTROL_IP" != "None" ] && [ "$CONTROL_IP" != "null" ]; then
              echo "Found oldest control plane node at ${CONTROL_IP}"
              break
            fi
            echo "Attempt ${i}/40: no running control node found yet..."
            sleep 15
          done

          if [ -z "$CONTROL_IP" ] || [ "$CONTROL_IP" = "None" ] || [ "$CONTROL_IP" = "null" ]; then
            echo "::error::No control plane node found within 10 minutes"
            exit 1
          fi

          # Wait for K3s to generate kubeconfig on the control node
          echo "Waiting for K3s kubeconfig on ${CONTROL_IP}..."
          for i in $(seq 1 40); do
            if ssh -o StrictHostKeyChecking=no -o ConnectTimeout=5 \
                   -o "ProxyCommand=${PROXY_CMD}" -i ${SSH_KEY} \
                   "ubuntu@${CONTROL_IP}" \
                   "sudo test -f /etc/rancher/k3s/k3s.yaml" 2>/dev/null; then
              echo "K3s kubeconfig found"
              break
            fi
            echo "Attempt ${i}/40: K3s not ready yet..."
            sleep 15
          done

          # Extract kubeconfig and replace localhost with LB DNS
          ssh -o StrictHostKeyChecking=no -o "ProxyCommand=${PROXY_CMD}" -i ${SSH_KEY} \
            "ubuntu@${CONTROL_IP}" "sudo cat /etc/rancher/k3s/k3s.yaml" | \
            sed "s|server: https://127.0.0.1:6443|server: https://${LB_DNS}:6443|" \
            > /tmp/kubeconfig.yaml

          # Copy to bastion
          scp -o StrictHostKeyChecking=no -i ${SSH_KEY} \
            /tmp/kubeconfig.yaml "ubuntu@${BASTION_IP}:~/kubeconfig.yaml"

          echo "::notice::Real K3s kubeconfig copied to bastion (via control node ${CONTROL_IP})"

      - name: Validate cluster health
        if: steps.kubeconfig.outcome == 'success'
        timeout-minutes: 15
        run: |
          BASTION_IP="${{ steps.bastion.outputs.bastion_ip }}"
          SSH_OPTS="-o StrictHostKeyChecking=no -i /tmp/bastion_ssh_key"

          bastion_kubectl() {
            ssh $SSH_OPTS "ubuntu@${BASTION_IP}" \
              "KUBECONFIG=~/kubeconfig.yaml kubectl $*"
          }

          echo "Waiting for cluster to be healthy (timeout: 15 minutes)"

          # Poll for at least 1 node Ready (15s intervals, up to 60 retries = 15 min)
          RETRY=0
          MAX_RETRY=60
          READY_NODES=0

          while [ $RETRY -lt $MAX_RETRY ]; do
            NODES=$(bastion_kubectl get nodes --no-headers 2>/dev/null || true)
            READY_NODES=$(echo "$NODES" | grep -c " Ready" || true)
            TOTAL_NODES=$(echo "$NODES" | grep -c "." || true)

            echo "Attempt $((RETRY + 1))/$MAX_RETRY: Nodes $READY_NODES/$TOTAL_NODES Ready"

            if [ "$READY_NODES" -ge 1 ]; then
              echo "At least one node is Ready"
              break
            fi

            RETRY=$((RETRY + 1))
            sleep 15
          done

          if [ "$READY_NODES" -lt 1 ]; then
            echo "::error::No nodes became Ready within 15 minutes"
            echo "=== Node Status ==="
            bastion_kubectl get nodes -o wide || true
            echo "=== Node Descriptions ==="
            bastion_kubectl describe nodes || true
            echo "=== System Events ==="
            bastion_kubectl get events -n kube-system --sort-by='.lastTimestamp' || true
            exit 1
          fi

          echo "Checking for coredns pods..."

          # Wait for coredns to be Running (5 minute timeout)
          if ! bastion_kubectl wait --for=condition=Ready pod -l k8s-app=kube-dns -n kube-system --timeout=300s; then
            echo "::error::coredns pods not Ready within 5 minutes"
            echo "=== kube-system Pods ==="
            bastion_kubectl get pods -n kube-system -o wide || true
            echo "=== coredns Pod Descriptions ==="
            bastion_kubectl describe pods -n kube-system -l k8s-app=kube-dns || true
            exit 1
          fi

          echo "Cluster is healthy!"

          # Write health summary
          READY_NODE_COUNT=$(bastion_kubectl get nodes --no-headers | grep -c " Ready" || true)
          COREDNS_COUNT=$(bastion_kubectl get pods -n kube-system -l k8s-app=kube-dns --no-headers | grep -c "Running" || true)

          cat >> $GITHUB_STEP_SUMMARY << EOF
          ### Cluster Health Validation

          | Check | Status |
          |-------|--------|
          | Nodes Ready | $READY_NODE_COUNT |
          | coredns Running | $COREDNS_COUNT |

          **Cluster:** test-k8s-${{ github.run_id }}
          **Load Balancer:** ${{ steps.k8s.outputs.load_balancer_dns }}
          EOF

      # ============================================================
      # Cleanup: Reverse order (Bastion -> K8s -> IAM -> VPC)
      # ============================================================
      - name: Bastion - Destroy
        if: always() && steps.bastion.outcome == 'success' && (github.event_name != 'workflow_dispatch' || github.event.inputs.skip_destroy != 'true')
        working-directory: tests/fixtures/bastion
        env:
          TF_VAR_vpc_id: ${{ steps.vpc.outputs.vpc_id }}
          TF_VAR_public_subnets: ${{ steps.vpc.outputs.public_subnets }}
          TF_VAR_cluster_security_group_id: ${{ steps.k8s.outputs.cluster_security_group_id }}
          TF_VAR_instance_type: ${{ vars.DEFAULT_INSTANCE_TYPE }}
          TF_VAR_ssh_public_key: ${{ steps.ssh_key.outputs.ssh_public_key }}
          TF_VAR_debug_ssh_public_key: ${{ vars.DEBUG_SSH_PUBLIC_KEY }}
        run: |
          echo "Destroying bastion host: test-k8s-${{ github.run_id }}-bastion"
          terraform destroy -auto-approve -input=false || {
            echo "::warning::Bastion destroy failed, continuing with K8s/IAM/VPC cleanup"
          }

      - name: Cleanup SSH key
        if: always()
        run: rm -f /tmp/bastion_ssh_key /tmp/bastion_ssh_key.pub /tmp/kubeconfig.yaml

      - name: K8s - Destroy
        if: always() && steps.k8s.outcome == 'success' && (github.event_name != 'workflow_dispatch' || github.event.inputs.skip_destroy != 'true')
        working-directory: tests/fixtures/k8s
        env:
          TF_VAR_vpc_id: ${{ steps.vpc.outputs.vpc_id }}
          TF_VAR_private_subnets: ${{ steps.vpc.outputs.private_subnets }}
          TF_VAR_iam_instance_profile: ${{ steps.iam.outputs.instance_profile_name }}
          TF_VAR_cluster_token: ${{ steps.token.outputs.token }}
          TF_VAR_default_instance_type: ${{ vars.DEFAULT_INSTANCE_TYPE }}
          TF_VAR_ssh_key_name: ${{ steps.bastion.outputs.ssh_key_name }}
          TF_VAR_debug_ssh_public_key: ${{ vars.DEBUG_SSH_PUBLIC_KEY }}
        run: |
          echo "Destroying K8s cluster: test-k8s-${{ github.run_id }}"
          terraform destroy -auto-approve -input=false || {
            echo "::warning::K8s destroy failed, continuing with IAM/VPC cleanup"
          }

      - name: IAM - Destroy
        if: always() && steps.iam.outcome == 'success' && (github.event_name != 'workflow_dispatch' || github.event.inputs.skip_destroy != 'true')
        working-directory: tests/fixtures/iam
        run: |
          echo "Destroying IAM resources: test-k8s-${{ github.run_id }}"
          terraform destroy -auto-approve -input=false || {
            echo "::warning::IAM destroy failed, continuing with VPC cleanup"
          }

      - name: VPC - Destroy
        if: always() && steps.vpc.outcome == 'success' && (github.event_name != 'workflow_dispatch' || github.event.inputs.skip_destroy != 'true')
        working-directory: tests/fixtures/vpc
        run: |
          echo "Destroying VPC: test-k8s-${{ github.run_id }}"
          if ! terraform destroy -auto-approve -input=false; then
            echo "::error::VPC destroy failed - manual cleanup may be required"
            cat >> $GITHUB_STEP_SUMMARY << EOF
          ### Manual Cleanup Required

          **VPC destroy failed.** Please manually clean up:
          - VPC Name: \`test-k8s-${{ github.run_id }}\`
          - Run ID: \`${{ github.run_id }}\`

          Check the zCompute console and delete resources with this name prefix.
          EOF
          fi

  summary:
    name: Integration Test Summary
    runs-on: ubuntu-latest
    needs: [check-credentials, test]
    if: always()
    steps:
      - name: Aggregate results
        id: aggregate
        run: |
          # Determine credential status
          HAS_CREDS="${{ needs.check-credentials.outputs.has_credentials }}"

          # Create summary
          cat >> $GITHUB_STEP_SUMMARY << 'EOF'
          ## Integration Test Results

          EOF

          if [ "$HAS_CREDS" != "true" ]; then
            cat >> $GITHUB_STEP_SUMMARY << 'EOF'
          ### Credentials Not Configured

          Integration tests were skipped because zCompute credentials are not configured.

          **To enable integration tests:**
          1. Add repository secrets: `ZCOMPUTE_ENDPOINT`, `ZCOMPUTE_ACCESS_KEY`, `ZCOMPUTE_SECRET_KEY`
          2. See [CONTRIBUTING.md](../CONTRIBUTING.md) for detailed setup instructions

          **Note:** This is expected for pull requests from forks.
          EOF
            echo "status=skipped" >> $GITHUB_OUTPUT
          else
            # Check test results
            TEST_RESULT="${{ needs.test.result }}"

            if [ "$TEST_RESULT" = "success" ]; then
              cat >> $GITHUB_STEP_SUMMARY << 'EOF'
          ### All Tests Passed

          K8s cluster successfully deployed, validated, and cleaned up.

          | Component | Status |
          |-----------|--------|
          | VPC | Created and destroyed |
          | IAM Instance Profile | Created and destroyed |
          | K8s Cluster | Created, validated, and destroyed |
          | Bastion Host | Created and destroyed |
          | Cluster Health | Nodes Ready, coredns Running |
          EOF
              echo "status=success" >> $GITHUB_OUTPUT
            else
              cat >> $GITHUB_STEP_SUMMARY << 'EOF'
          ### Tests Failed

          Integration test failed. Check individual job logs for details.

          **Possible issues:**
          - VPC creation failed
          - IAM instance profile creation failed
          - K8s cluster creation failed
          - Cluster health validation failed
          - Resource cleanup failed
          EOF
              echo "status=failed" >> $GITHUB_OUTPUT
            fi
          fi

      - name: Post PR comment
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v7
        with:
          script: |
            const marker = '<!-- k8s-integration-test-results -->';
            const hasCreds = '${{ needs.check-credentials.outputs.has_credentials }}' === 'true';
            const status = '${{ steps.aggregate.outputs.status }}';

            let body = `${marker}\n`;

            if (!hasCreds) {
              body += `## Integration Tests Skipped

            zCompute credentials are not configured for this repository.

            **This is expected for pull requests from forks.** Maintainers will run integration tests before merging.

            <details>
            <summary>How to enable integration tests</summary>

            Repository maintainers can add these secrets in Settings > Secrets and variables > Actions:
            - \`ZCOMPUTE_ENDPOINT\` - zCompute region API endpoint
            - \`ZCOMPUTE_ACCESS_KEY\` - AWS-style access key
            - \`ZCOMPUTE_SECRET_KEY\` - AWS-style secret key

            See [CONTRIBUTING.md](../CONTRIBUTING.md) for details.
            </details>
            `;
            } else if (status === 'success') {
              body += `## Integration Tests Passed

            K8s module successfully deployed and validated against zCompute infrastructure.

            | Component | Status |
            |-----------|--------|
            | VPC | Created and destroyed |
            | IAM Instance Profile | Created and destroyed |
            | K8s Cluster | Created, validated, and destroyed |
            | Bastion Host | Created and destroyed |
            | Cluster Health | Nodes Ready, coredns Running |

            [View workflow run](https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }})
            `;
            } else {
              body += `## Integration Tests Failed

            K8s module integration tests failed. Please check the logs.

            [View workflow run](https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }})
            `;
            }

            // Find existing comment
            const {data: comments} = await github.rest.issues.listComments({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: context.issue.number
            });

            const botComment = comments.find(c => c.body.includes(marker));

            if (botComment) {
              // Update existing comment
              await github.rest.issues.updateComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                comment_id: botComment.id,
                body: body
              });
            } else {
              // Create new comment
              await github.rest.issues.createComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: context.issue.number,
                body: body
              });
            }

      - name: Fail if tests failed
        if: steps.aggregate.outputs.status == 'failed'
        run: |
          echo "::error::Integration tests failed"
          exit 1
